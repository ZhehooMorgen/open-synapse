# 人工智能应当是一个基于启发式搜索的NP问题求解器

本文最新版本发布在 https://github.com/ZhehooMorgen/open-synapse/blob/main/projects/NPSolver/Theory.en.md

在[Readme](../../README.md)中，我们已经指出，当前AI技术落地的两大问题是：

1. 缸中之脑问题
2. 模型的局限性问题

其中缸中之脑的问题已经在Readme中有了比较详细的讨论，而模型的局限性问题我们在Readme中只是简单提了一下，认为这是一个需要全新思路才能解决的问题。

模型的局限性问题，其实也就是输出质量的问题，我们可以细分为好几个问题：

1. 模型会有幻觉，无法区分内容可靠性。
2. 模型无法进行复杂的推理和规划。
3. 模型的上下文窗口有限，无法记住长期记忆。
4. 模型可能会被prompt injection攻击，导致严重的安全漏洞。
5. 模型的内部的工作机制是一个黑盒，我们无法完全理解它的工作原理和机制，这也导致我们无法完全信任它的输出。
6. 还存在非常多的其他问题，问题太多了，我们无法在此完全列举。

这篇文档将会通过分析AI模型的本质，从本质出发来解决模型的局限性问题。首先我们从经典的NP问题说起，指出AI模型的本质就是一个启发式状态空间NP问题求解器。接着，基于这个理论，我们可以分析出当前AI模型和工具在训练和使用AI模型时存在的缺陷，并且我们可以基于这个理论来设计出一个全新的AI模型和工具的架构，这个架构可以在很大程度上解决当前AI模型能力不足的问题。
本文档为了叙述的方便性和清晰性，一定程度上牺牲了数学上的严谨性，来更直观的表达这个理论和相关的设计。严谨的学术性论文的写作和发表是我们未来的工作计划。

## AI模型是一个启发式状态空间NP问题求解器

### 从一个现象说起

当我们要求LLM完成一项高难度的分析任务时，直接提问的效果往往令人失望。但如果在prompt中加入一步引导——例如，要求模型先问自己"应该用什么思路来分析这个问题"——输出质量往往会有显著提升。

这个现象背后隐藏着一个关键问题：既然模型在引导下能给出更好的答案，那么这种更优解法的能力，原本就存在于模型之中。为什么模型一开始不这么做？

要回答这个问题，我们先需要理解AI模型在做什么。

### 搜索即生成

在计算复杂性理论中，P问题是指可以在多项式时间内求解的问题，而NP问题是指可以在多项式时间内**验证**一个给定解是否正确的问题。需要说明的是，本文对P和NP的使用是非正式的类比，与严格的数学定义存在出入：严格意义上的P和NP是针对判定问题（decision problem）的复杂性类，且P⊆NP已被证明成立，而P≠NP至今仍是未解的猜想。本文借用这一框架作为直觉性的描述工具，而非严谨的数学论断。两类问题的本质区别在于：找到解与核验解，往往难度相差悬殊。面对NP问题，计算机科学给出的实用方案是**启发式搜索**——不追求全局最优解，而是通过某种启发函数引导搜索方向，在可接受的时间内找到足够好的解。

LLM的生成过程正是如此。当模型回答一个问题时，它并不是从某个知识库中"查找"答案，而是在一个近乎无穷的token序列空间中，依据训练习得的概率分布，逐步采样出一段文字。这个过程的本质，是在巨大的状态空间中推进的搜索——每一个token的生成，都是从当前状态出发、向下一个状态的转移。

**LLM就是一个启发式状态空间NP问题求解器。** 它的"启发函数"由训练数据和权重参数编码，它的"搜索策略"由自回归生成机制决定。所有生成式AI模型，甚至包括人类的意识，都可以被理解为在某个状态空间中进行启发式搜索的求解器。无论是那种按部就班的Chain-of-Thought，还是那种看似灵光一现的"灵感"，都是在这个搜索空间中不同方式的路径选择。所谓灵感，可以理解为：通过反思已经探索过的搜索空间，大胆推断在未探索的某个区域可能存在质量更高的解，从而将搜索器引向那个方向进行尝试。它不是凭空出现的，而是已有搜索经验的归纳与外推。任何智能体的思考，从试探、观察到总结，本质上就是在无穷搜索空间中，尽力迈向最优解的过程。

需要指出的是，寻找解的（一阶问题）难度和寻找寻找解的方法（二阶问题）本质上都是属于NP问题，而且前者并不必然比后者复杂，后者也不必然比前者复杂。甚至给定一个问题，证明其一阶与其衍生的高阶问题哪个更简单或者复杂本身也是NP问题。对于生成式AI来说，所有的问题本质上都处于同一个无穷的搜索空间中，搜索高阶问题的解本身也是搜索低阶问题解的可选路径，反之亦然。因此本文认为，在研究利用生成式AI来解决实际问题的领域，**刻意地区分低阶问题和其衍生的高阶问题是没有意义的**，很多时候其实可以不需要区分他们，本文中主要以不区分的方式处理。

### 为什么Prompt技巧有效

回到最初的问题：为什么引导性prompt能显著提升输出质量？

因为prompt改变了搜索路径。自回归生成本质上是贪心解码——模型在当前状态下选择概率最高的下一步，而不是从全局评估哪条路径最终会抵达最优解。引导性prompt相当于为搜索器提前注入了启发信息，使搜索过程经过更有价值的中间状态，从而更大概率地抵达高质量的输出区域。

Chain-of-Thought、self-questioning、think-step-by-step……这些技术在形式上各有不同，但本质相同：它们都是在调整搜索路径。模型并非因为"被引导"才变得"更聪明"，而是因为更好的搜索路径，激活了参数空间中本已潜在的求解能力。

### 智能体的根本局限性

在论证了智能体，包括人类和AI，本质是一个NP求解器之后，人类使用AI这件事的逻辑就变得非常清晰：把问题交给AI，是把NP的生成负担转嫁给机器；而人类自己只需评估生成的解是否可接受。验证一个解是否合格，通常远比从头生成一个解容易——这更接近于P类问题。

因此，使用AI的根本目的，是将不确定性问题中生成解的负担外包，而自己承担相对轻松的验证工作。

又因为任何智能体，包括人类和AI，本质上都是用有限的资源在一个巨大的搜索空间中寻找解的求解器之后，我们就必须接受一个现实：任何智能体都只能在某个特定的搜索空间中，基于某种启发式策略，寻找足够好的解。我们无法要求一个算力和时间有限的系统，总是能够使用启发式的搜索策略，在无穷的状态空间中，找到全局最优解。我们只能要求它在合理的资源约束下，以一个较为优化的策略进行搜索，以返回一个足够好的结果。**不存在某种技术，可以让AI总是找到足够好的解。**

## 基于启发式状态空间NP问题求解器理论的AI模型和工具设计的必要性

**人类在知道自己无法成为神之后，才真正开始建设属于自己的文明。人类在知道AI无法成为神之后，才真正用其建造通向未来的阶梯。**

本文前半部分的论述已经揭示了AI模型的本质，也指出了在这个框架下，包括AI和人类在内的所有智能体的能力上限，**否定了完美的，不会犯错的AI的存在可能性。**这并非是一个悲观的结论，而是一个解放性的真相。它让我们不再对AI抱有不切实际的幻想，也不再被那些关于"失控的强人工智能"的恐怖故事所吓倒。相反，我们可以基于这个理论，设计出一个更符合现实、能够在实际应用中发挥更大价值的AI模型和工具架构。

### 当前AI技术的固有缺陷

前文我们论证了任何智能体本质上都是启发式搜索器，但当前的AI实现方式却并未遵循这一理想架构。现有的LLM，基于LLM的工具，以及非LLM的各种生成式AI系统，虽然在某些特定任务上表现出色，但是无法克服文章开头的那些局限性。这些缺陷是根植于它们的架构设计中的，而非简单的训练数据或模型规模问题：

1. **启发，搜索，评估，和输出被耦合为单一推理过程**：由于模型的不可解释性，我们甚至无法确认真的存在一个内部的启发和评估过程，或者每次推理都确实进过了这些步骤。即便存在，它们也只是前向传播的副产物，而非显式设计的模块。这些本该分工明确的步骤，被强行捆绑在一个不可拆解的黑箱里，既无法从外部观察，也无法被干预或验证。因此，当前的AI系统虽然在形式上执行了搜索，但在本质上并未遵循“显式启发-多路径探索-结果验证”的规范求解器架构。 它不是一个合理运作的启发式NP搜索求解器，而是一个将求解过程全部压缩进单次前向传播的“快餐式”实现。上述种种问题——幻觉、推理断裂、对提示词的过度敏感——正是这种耦合架构的必然产物。

2. **生成式架构和当前的训练模式都更倾向于复杂的输出**：在实际应用中，生成式模型往往倾向于产生较为复杂、详尽的输出，这在一定程度上增加了人类验证的负担。实际上这一问题并非AI独有，例如在软件开发领域，如果将编写代码的工作完全外包给第三方，那么委托方就会面临一个问题：他们需要验证交付的代码是否正确，但如果代码过于复杂，那么验证本身就会变成一个NP问题，导致效率极低。AI生成的内容也是如此，如果生成的解过于复杂，那么验证它是否正确也会变得非常困难，甚至可能无法在合理的时间内完成。这种倾向于复杂输出的架构设计，无疑加剧了验证的难度。这一方面使得之前提到的将"NP的生成负担外包"的效率提升逻辑被破坏了，另一方面也使得AI内部应有的评估机制无法有效地发挥作用。

3. **过度堆砌规模和算力，以规模扩张掩盖架构缺陷**：面对上述根本性问题，产业界的主流应对是不断堆砌模型参数和训练算力，试图通过Scaling Law“暴力破解”智能。然而，模型规模的扩大固然能提升启发函数的精度，但其边际收益正急剧递减——更大的模型依然困于耦合的黑箱，幻觉与推理断裂并未消失。更重要的是，这种单一维度的堆砌忽略了更高效的替代路径：缩小单次搜索迭代的计算量（采用适度规模的模型），并将算力转移到推理阶段的多方向、多路径搜索中。正如“用计算换参数”所揭示的，只要搜索次数m小于模型缩小的倍数k，小模型+深度搜索就能在更低成本下达到甚至超越大模型的效果。当前过度堆砌规模的路线，本质上是用昂贵的训练算力掩盖了搜索策略的缺失，是对“启发式搜索”本质的误解。

### 理想的基于AI的启发式状态空间NP问题求解器需要具备的特征

既然当前系统的缺陷根源于“启发、搜索、评估、输出”的深度耦合，那么理想的求解器必然走向另一条路：显式解耦与模块化分工。一个合理运作的启发式搜索求解器，应当具备以下核心特征：

1. **显式解耦与模块化分工**：显式区分启发、搜索、评估等不同功能。
2. **多路径管理**：启发器应当同时提出多个可能的搜索方向，避免过早收敛到一个狭窄的路径上，从而错失更优解。同时又要避免过多的分支导致搜索空间过大，试图以有限算力去抗衡无穷的状态空间。
3. **独立的评估器**：评估不再隐含在每一步的token概率中，而是由一个专门的模块对完整或部分的候选解进行打分。
4. **在机制式上倾向于更简单的解**：解的复杂度应当尽可能低，以便于验证和理解，同时减少搜索空间的膨胀。
5. **可解释性**：整个求解过程应当留下足够的中间记录，使得人类能够理解和追溯模型的推理路径。

## 基于启发式状态空间NP问题求解器理论的AI系统设计

### 设计目标

基于以上约束，我们对这个求解器的成功有了明确的度量标准：**在给定的算力和时间约束下，通过合理的步骤找到高质量的解**。

### 模块设计

需要指出的是，以下的模块设计，并非是说我们需要为每个模块训练一个独立的模型来实现它们的功能。相反，我们只是希望在设计上明确保证这些功能一定存在并且能在运行中一定能够被调用到。我们既可以用多agent的方式来实现这些模块的分工，也可以修改推理架构来实现这些模块的功能。总之，我们在这个章节只是想要讨论他们的功能和设计，而不是讨论他们的具体实现方式。具体的实现方式，我们会在后续的章节中进行讨论。

**启发器（Heuristic）**

负责分析当前的问题和搜索状态，判断哪些方向最有可能存在优质解。这对应人类思维中的元认知——先问自己"应该怎么思考这个问题"，再开始思考。

启发器不应只给出单一方向，而应同时提出多个可能的思路。不同思路代表搜索空间中的不同分支，后续可由编排器并发展开，充分利用计算资源，避免因过早收窄方向而错失更优解。

**搜索器（Searcher）**

在启发器提供的方向引导下生成候选解。在新架构中，它不再是无方向的单次生成，而是在指定方向上的定向探索。

当搜索器发现当前方向过于模糊或分支过多时，可以递归地调用启发器，对该子方向进一步细化，再继续推进。这种机制自然地形成多级树状搜索结构：每个节点可以展开为新的子方向，搜索深度和广度都可以根据资源约束动态调整。

**细化器（Refiner）**

对候选解进行迭代细化和修正。搜索器产出的可能是一个粗糙但方向正确的候选解，细化器负责将其逐步打磨为质量更高、更完整的解。这个过程可以多轮进行。

**评估器（Evaluator）**

对候选解进行多维度评估，为后续的决策提供依据。评估维度至少应包括：

1. **解的复杂度**：过于复杂的解可能是过拟合，并且会导致验证本身变成NP问题，直接破坏"人负责验证"的效率假设。好的解应当在满足需求的前提下尽可能简洁。
2. **可解释性**：解的推理链是否清晰，人类是否能够理解和追溯其得出过程。
3. **依据的可考察性**：解中涉及的知识点、引用的事实和数据，是否具备可验证的来源。
4. **其他约束**：根据应用场景，可扩展加入版权合规性、内容安全性等维度的检查。

需要注意的是，以上的评估动作，并非真的用某个算法来进行实际的评估，而是利用例如AI模型等算法进行启发式验证。可以发现，我们故意避开了正确性维度的评估，而是看简洁性，可解释性等指标。因为我们知道，答案的评估本身可能就是一个NP问题，如果评估器试图给出一个绝对的判断，那么它自己就会陷入效率危机，无法胜任"验证"的角色。所以评估器更多是看这个解“长得好不好看”。

评估器的输出是结构化的评分与分析，而不仅仅是一个通过/不通过的判断。这些信息将交由决策与整合器处理。

**决策与整合器（Decision & Integrator）**

在多个并发搜索分支产生各自的候选解后，负责综合评估结果做出最终决策。其工作包含三种情形：

- **择优返回**：若某个分支的候选解在各维度评估中显著优于其他分支，直接选取该解输出。
- **整合多解**：若多个分支均产生了有价值的结论且相互补充，将其合并为一个更完整的答案。
- **启动下一轮搜索**：若当前所有分支的结果均不令人满意，对本轮搜索中获得的有效信息进行总结，提炼出"哪些方向已被排除"、"哪些发现值得深入"等结构化洞察，作为下一轮启发器的输入，以新的策略重新展开搜索。

这使得整个系统不只是单次搜索，而是一个可以从失败中学习、持续迭代的搜索循环。

## 基于启发式状态空间NP问题求解器理论的AI系统实现

为了实现上述的设计，我们可以在模型架构、训练方法、推理机制等多个层面进行创新。以下列出一些思路：

### 训练模型法

通过以强化学习为代表的各种方式对LLM模型进行训练，使其自动按照上述的工作流程来进行推理和生成。然而，上文中已经对使用单一模型来实现所有功能的架构设计提出了批评，认为它过于耦合，缺乏模块化分工，因此我们并不推荐这种方式。

### 修改模型架构法

通过修改模型的架构设计来实现上述的功能分工。例如，我们可以在模型中引入显式的模块来分别处理启发、搜索、评估等功能，或者设计一种新的推理机制，使得这些功能能够在推理过程中被调用和执行。这种方式虽然在设计上更符合我们对启发式搜索求解器的理解，但在实际实现中可能会面临较大的技术挑战，需要对现有的模型架构进行较大幅度的改动。

### 修改推理机制法

通过修改底层的推理机制来实现上述的功能分工。例如，通过修改llama.cpp等推理框架的代码，将启发、搜索、评估等功能的工作流循环嵌入到推理过程中，使得模型在生成过程中能够按照我们设计的流程来进行推理和生成。这种方式相对于修改模型架构来说，可能更容易实现一些，因为它不需要对模型本身进行大幅度的改动，而是通过调整推理过程中的工作流来实现功能分工。甚至可以直接使用现有的开源模型，从而大幅简化实现过程。

### 多Agent协作法

通过设计多个专门的Agent来分别负责启发、搜索、评估等功能，并通过一个编排器来协调它们的工作。这种方式在设计上非常符合我们对启发式搜索求解器的理解，并且在实际实现中也相对可行。每个Agent可以使用现有的模型来实现其功能，而编排器则负责协调它们的工作流程，使得整个系统能够按照我们设计的流程来进行推理和生成。甚至可以为这些Agent添加知识库检索等功能，从而进一步提升它们的能力。但是对比修改推理机制法来说，这种方式可能会引入更多的通信开销。

## 为什么这个设计可以解决当前AI模型能力不足的问题

引言中列出了模型能力不足的若干具体表现。在NP求解器的架构下，这些问题的性质发生了根本性的转变。

**幻觉问题**

在传统的单次生成中，幻觉是悄无声息地混入最终输出的错误，用户无从感知。在新架构中，候选解首先由评估器审查，幻觉从"无法感知的输出错误"转变为"系统内部可处理的搜索噪声"。它不会消失，但其影响范围被限制在系统内部，不再直接暴露给用户。

**复杂推理和规划问题**

复杂任务之所以难以一次性完成，是因为需要经过的中间步骤太多，单次贪心解码很容易走偏。启发器提供方向、搜索器定向生成、细化器迭代修正的多步机制，将复杂推理分解为一系列有方向的短程搜索，从结构上缓解了这个问题。

**不可解释性问题**

编排式工作流天然留下了中间记录：启发器的分析推断、搜索器的候选解、评估器的判断依据。这使得模型的推理过程具备了一定程度的可追溯性，从而部分缓解了"黑盒"问题。

**关于幻觉、道德性与失控AI**

将AI定位为NP求解器，从根本上重塑了我们对这些问题的理解。P≠NP保证了不存在可以高效解决所有NP问题的通用算法，这意味着AI永远是一个受约束的搜索引擎，而非一个逼近全能的通用智能体。

在这个框架下，所谓"失控的强人工智能"所依赖的前提——AI能够自主地在任意领域找到任意问题的最优解——在理论上就是不成立的。一个被明确设计为"在约束下搜索解"的系统，其行为边界比一个试图模拟通用智能的系统要清晰得多，也可信得多。

## 总结

本文将生成式AI模型重新概念化为**启发式状态空间NP问题求解器**：其生成过程是在无穷搜索空间中，基于参数编码的启发函数进行自回归搜索。这一框架将AI与人类智能统一在有限资源约束下的求解范式之中。

基于此理论，我们揭示了当前AI架构的根本缺陷：启发、搜索、评估与输出被耦合为单一前向传播，导致系统成为不可观察、不可干预的黑箱；而产业界以堆砌算力掩盖架构问题，忽略了将算力转向多路径搜索这一更高效的替代路径。

我们进而提出一种显式解耦的模块化求解器架构——由启发器、搜索器、评估器、决策器等协同工作，使AI从“快餐式生成器”转变为可追溯、可干预的协作工具。该设计从根源上缓解了幻觉、推理断裂等问题，并为可解释AI提供了自然支撑。

理论层面，本文从计算复杂性角度**否定了“完美AI”的存在可能**，转而**将AI从神话还原为平凡的工程对象，为理性设计AI工具奠定了认知基础**。

未来工作将聚焦于该架构的具体实现路径（如修改推理机制、多Agent协作），并将其推广至多模态任务与复杂决策场景。我们相信，基于搜索的视角将引领AI走向更可靠、更高效的实用化道路。
